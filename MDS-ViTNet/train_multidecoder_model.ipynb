{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Z3dyFZET9P90"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import bitsandbytes as bnb\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.loss_function import SaliencyLoss\n",
    "from utils.loss_function import AUC\n",
    "from utils.data_process import MyDataset, MyTransform\n",
    "from utils.data_process import MyDatasetCNNMerge, MyTransformCNNMerge\n",
    "from utils.data_process import preprocess_img, postprocess_img\n",
    "from utils.data_process import compute_metric, compute_metric_CNNMerge\n",
    "from utils.data_process import count_parameters\n",
    "\n",
    "from model.Merge_CNN_model import CNNMerge\n",
    "\n",
    "flag = 3 # 0 for TranSalNet_Dense, 1 for TranSalNet_Res, 2 for TranSalNet_ViT, 3 for TranSalNet_ViT_multidecoder\n",
    "\n",
    "if flag == 0:\n",
    "    from model.TranSalNet_Dense import TranSalNet\n",
    "elif flag == 1:\n",
    "    from model.TranSalNet_Res import TranSalNet\n",
    "elif flag == 2:\n",
    "    from model.TranSalNet_ViT import TranSalNet\n",
    "elif flag == 3:\n",
    "    from model.TranSalNet_ViT_multidecoder import TranSalNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OnIDauBqVlKY"
   },
   "outputs": [],
   "source": [
    "path_images_train = './datasets/train/train_images/'\n",
    "path_images_val = './datasets/val/val_images/'\n",
    "path_images_test = './datasets/test/test_images_salicon/'\n",
    "\n",
    "path_maps_train = './datasets/train/train_maps/'\n",
    "path_maps_val = './datasets/val/val_maps/'\n",
    "\n",
    "path_train_ids = './datasets/train_ids_SALICON_CAT.csv'\n",
    "path_val_ids = './datasets/val_ids_SALICON_CAT.csv'\n",
    "\n",
    "print(len(os.listdir(path_images_train)))\n",
    "print(len(os.listdir(path_images_test)))\n",
    "print(len(os.listdir(path_images_val)))\n",
    "print(len(os.listdir(path_maps_train)))\n",
    "print(len(os.listdir(path_maps_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = pd.read_csv(path_train_ids)\n",
    "val_ids = pd.read_csv(path_val_ids)\n",
    "print(train_ids.iloc[1])\n",
    "print(val_ids.iloc[1])\n",
    "\n",
    "dataset_sizes = {'train': len(train_ids), 'val': len(val_ids)}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MNqBX79-VlKY"
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "shape_r = 288\n",
    "shape_c = 384\n",
    "\n",
    "p = 0.5\n",
    "train_transform = MyTransform(p=p, shape_r=shape_r, shape_c=shape_c, iftrain=True)\n",
    "val_transform = MyTransform(p=p, shape_r=shape_r, shape_c=shape_c, iftrain=False)\n",
    "\n",
    "train_set = MyDataset(\n",
    "    ids=train_ids,\n",
    "    stimuli_dir=path_images_train,\n",
    "    saliency_dir=path_maps_train,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "val_set = MyDataset(\n",
    "    ids=val_ids,\n",
    "    stimuli_dir=path_images_val,\n",
    "    saliency_dir=path_maps_val,\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "dataloaders = {\n",
    "    'train':DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "    'val':DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TranSalNet()\n",
    "model = model.to(device)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZR2aj0i9P96"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "H6dseWckHh_Y"
   },
   "outputs": [],
   "source": [
    "history_loss_train = []\n",
    "history_loss_val = []\n",
    "\n",
    "history_loss_train_cc = []\n",
    "history_loss_train_sim = []\n",
    "history_loss_train_kldiv = []\n",
    "history_loss_train_nss = []\n",
    "history_loss_train_auc = []\n",
    "\n",
    "history_loss_val_cc = []\n",
    "history_loss_val_sim = []\n",
    "history_loss_val_kldiv = []\n",
    "history_loss_val_nss = []\n",
    "history_loss_val_auc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = bnb.optim.Adam8bit(model.parameters(), lr=1e-5)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "loss_fn = SaliencyLoss()\n",
    "\n",
    "'''Training'''\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "num_epochs = 30\n",
    "best_loss = 100\n",
    "path_to_save = 'path to save'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()  # Set model to training mode\n",
    "        else:\n",
    "            model.eval()   # Set model to evaluate mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for i_batch, sample_batched in tqdm(enumerate(dataloaders[phase])):\n",
    "            stimuli, smap = sample_batched['image'], sample_batched['saliency']\n",
    "            stimuli, smap = stimuli.type(torch.float32), smap.type(torch.float32)\n",
    "            stimuli, smap = stimuli.to(device), smap.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                if flag == 3:\n",
    "                    outputs_1, outputs_2 = model(stimuli)\n",
    "                    \n",
    "                    loss_1 = -2*loss_fn(outputs_1, smap, loss_type='cc')\n",
    "                    loss_1 = loss_1 - loss_fn(outputs_1, smap, loss_type='sim')\n",
    "                    loss_1 = loss_1 + 10*loss_fn(outputs_1, smap, loss_type='kldiv')\n",
    "                    \n",
    "                    loss_2 = -2*loss_fn(outputs_2, smap, loss_type='cc')\n",
    "                    loss_2 = loss_2 - loss_fn(outputs_2, smap, loss_type='sim')\n",
    "                    loss_2 = loss_2 + 10*loss_fn(outputs_2, smap, loss_type='kldiv')\n",
    "                    \n",
    "                    loss = loss_1 + loss_2\n",
    "                else:\n",
    "                    outputs = model(stimuli)\n",
    "\n",
    "                    loss = -2*loss_fn(outputs, smap, loss_type='cc')\n",
    "                    loss = loss - loss_fn(outputs, smap, loss_type='sim')\n",
    "                    loss = loss + 10*loss_fn(outputs, smap, loss_type='kldiv')\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            if phase == 'train':\n",
    "                if flag == 3:\n",
    "                    history_loss_train.append(loss.item())\n",
    "                    history_loss_train_cc.append([loss_fn(outputs_1, smap, loss_type='cc').item(), loss_fn(outputs_2, smap, loss_type='cc').item()])\n",
    "                    history_loss_train_sim.append([loss_fn(outputs_1, smap, loss_type='sim').item(), loss_fn(outputs_2, smap, loss_type='sim').item()])\n",
    "                    history_loss_train_kldiv.append([loss_fn(outputs_1, smap, loss_type='kldiv').item(), loss_fn(outputs_2, smap, loss_type='kldiv').item()])\n",
    "                    history_loss_train_nss.append([loss_fn(outputs_1, smap, loss_type='nss').item(), loss_fn(outputs_2, smap, loss_type='nss').item()])\n",
    "                else:\n",
    "                    history_loss_train.append(loss.item())\n",
    "                    history_loss_train_cc.append(loss_fn(outputs, smap, loss_type='cc').item())\n",
    "                    history_loss_train_sim.append(loss_fn(outputs, smap, loss_type='sim').item())\n",
    "                    history_loss_train_kldiv.append(loss_fn(outputs, smap, loss_type='kldiv').item())\n",
    "                    history_loss_train_nss.append(loss_fn(outputs, smap, loss_type='nss').item())\n",
    "            else:\n",
    "                if flag == 3:\n",
    "                    history_loss_val.append(loss.item())\n",
    "                    history_loss_val_cc.append([loss_fn(outputs_1, smap, loss_type='cc').item(), loss_fn(outputs_2, smap, loss_type='cc').item()])\n",
    "                    history_loss_val_sim.append([loss_fn(outputs_1, smap, loss_type='sim').item(), loss_fn(outputs_2, smap, loss_type='sim').item()])\n",
    "                    history_loss_val_kldiv.append([loss_fn(outputs_1, smap, loss_type='kldiv').item(), loss_fn(outputs_2, smap, loss_type='kldiv').item()])\n",
    "                    history_loss_val_nss.append([loss_fn(outputs_1, smap, loss_type='nss').item(), loss_fn(outputs_2, smap, loss_type='nss').item()])\n",
    "                else:\n",
    "                    history_loss_val.append(loss.item())\n",
    "                    history_loss_val_cc.append(loss_fn(outputs, smap, loss_type='cc').item())\n",
    "                    history_loss_val_sim.append(loss_fn(outputs, smap, loss_type='sim').item())\n",
    "                    history_loss_val_kldiv.append(loss_fn(outputs, smap, loss_type='kldiv').item())\n",
    "                    history_loss_val_nss.append(loss_fn(outputs, smap, loss_type='nss').item())\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        if phase == 'train':\n",
    "            scheduler.step()\n",
    "\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "        print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "\n",
    "        if phase == 'val' and epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            counter = 0\n",
    "        elif phase == 'val' and epoch_loss >= best_loss:\n",
    "            counter += 1\n",
    "            if counter == 5:\n",
    "                savepath = path_to_save + '/TranSalNet_ViT_multidecoder_'+str(epoch)+'.pth'\n",
    "                torch.save(model.state_dict(), savepath)\n",
    "                print('EARLY STOP!')\n",
    "                break\n",
    "\n",
    "    # saving weights\n",
    "    if epoch%1 == 0:\n",
    "        savepath = path_to_save + '/TranSalNet_ViT_multidecoder_'+str(epoch)+'.pth'\n",
    "        torch.save(model.state_dict(), savepath)\n",
    "\n",
    "    print()\n",
    "\n",
    "print('Best val loss: {:4f}'.format(best_loss))\n",
    "savepath = path_to_save + '/TranSalNet_ViT_multidecoder_'+str(epoch)+'.pth'\n",
    "torch.save(model.state_dict(), savepath)\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geurEf_d_koh"
   },
   "source": [
    "# Show val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jBf7Ihie3Evk"
   },
   "outputs": [],
   "source": [
    "def make_sub(model, num_pic, flag, shape_r=288, shape_c=384):\n",
    "    im_path = './datasets/val_images/COCO_val2014_'+num_pic+'.jpg'\n",
    "    smap_path = './datasets/val_maps/COCO_val2014_'+num_pic+'.png'\n",
    "\n",
    "    image = Image.open(im_path).convert('RGB')\n",
    "    img = np.array(image) / 255.\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    img = torch.from_numpy(img)\n",
    "\n",
    "    saliency = Image.open(smap_path)\n",
    "    smap = np.expand_dims(np.array(saliency) / 255., axis=0)\n",
    "    smap = torch.from_numpy(smap)\n",
    "\n",
    "    img, smap = val_transform(img, smap)\n",
    "    img = img.type(torch.float32).to(device)\n",
    "    \n",
    "    toPIL = transforms.ToPILImage()\n",
    "\n",
    "    if flag == 3:\n",
    "        pred_1, pred_2 = model(img.unsqueeze(0))\n",
    "        pic_1 = toPIL(pred_1.squeeze())\n",
    "        pic_2 = toPIL(pred_2.squeeze())\n",
    "        pred_np_1 = pred_1.squeeze().detach().cpu().numpy()\n",
    "        pred_np_2 = pred_2.squeeze().detach().cpu().numpy()\n",
    "        smp_np = smap.squeeze().numpy()\n",
    "        auc_1 = AUC(pred_np_1, smp_np)\n",
    "        auc_2 = AUC(pred_np_2, smp_np)\n",
    "        return (pic_1, pic_1), (auc_1, auc_2)\n",
    "    else:\n",
    "        pred = model(img.unsqueeze(0))\n",
    "        pic = toPIL(pred.squeeze())\n",
    "        pred_np = pred.squeeze().detach().cpu().numpy()\n",
    "        smp_np = smap.squeeze().numpy()\n",
    "        auc = AUC(pred_np, smp_np)\n",
    "        return pic, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "path_sub_model = 'your path to model here'\n",
    "model.load_state_dict(torch.load(path_sub_model))\n",
    "model.eval()\n",
    "\n",
    "if flag == 3:\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\n",
    "else:\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "num_pic = str('000000010138')\n",
    "\n",
    "img_orig = mpimg.imread('./datasets/val_images/COCO_val2014_'+num_pic+'.jpg')\n",
    "img_true = mpimg.imread('./datasets/val_maps/COCO_val2014_'+num_pic+'.png')\n",
    "\n",
    "if flag == 3:\n",
    "    pics, aucs = make_sub(model, num_pic, flag=flag)\n",
    "    pic_1, pic_2 = pics\n",
    "    auc_1, auc_2 = aucs\n",
    "else:\n",
    "    pic, auc = make_sub(model, num_pic, flag=flag)\n",
    "\n",
    "if flag == 3:\n",
    "    ax[0][0].imshow(img_orig)\n",
    "    ax[0][0].set_title('Image')\n",
    "    \n",
    "    ax[0][1].imshow(img_true)\n",
    "    ax[0][1].set_title('True')\n",
    "    \n",
    "    ax[1][0].imshow(pic_1)\n",
    "    ax[1][0].set_title('Pred 1')\n",
    "    \n",
    "    ax[1][1].imshow(pic_2)\n",
    "    ax[1][1].set_title('Pred 2')\n",
    "else:\n",
    "    ax[0].imshow(img_orig)\n",
    "    ax[0].set_title('Image')\n",
    "    ax[1].imshow(pic)\n",
    "    ax[1].set_title('Pred')\n",
    "    ax[2].imshow(img_true)\n",
    "    ax[2].set_title('True')\n",
    "\n",
    "plt.show()\n",
    "if flag == 3:\n",
    "    print('AUC_1 = ', auc_1)\n",
    "    print('AUC_2 = ', auc_2)\n",
    "else:\n",
    "    print('AUC = ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFDCtgvM3Evk"
   },
   "source": [
    "# Compute val metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sub_model = 'your path to model here'\n",
    "model.load_state_dict(torch.load(path_sub_model))\n",
    "model.eval()\n",
    "\n",
    "val_loss, val_loss_cc, val_loss_sim, val_loss_kldiv, val_loss_nss, val_auc = compute_metric(\n",
    "    model, \n",
    "    dataloaders['val'], \n",
    "    device = device,\n",
    "    flag=flag, \n",
    "    t=10\n",
    ")\n",
    "\n",
    "print('Loss = ', val_loss)\n",
    "print('CC = ', val_loss_cc)\n",
    "print('SIM = ', val_loss_sim)\n",
    "print('KL = ', val_loss_kldiv)\n",
    "print('NSS = ', val_loss_nss)\n",
    "print('AUC = ', val_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create maps dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_weight = 'your path to model here'\n",
    "path_to_save_1 = './datasets/train/maps_train_1/' # create folder maps_train_1 before\n",
    "path_to_save_2 = './datasets/train/maps_train_2/' # create folder maps_train_2 before\n",
    "model.load_state_dict(torch.load(path_to_weight))\n",
    "model.eval()\n",
    "\n",
    "for i_batch, sample_batched in tqdm(enumerate(dataloaders['train'])):\n",
    "    stimuli, smap = sample_batched['image'], sample_batched['saliency']\n",
    "    stimuli, smap = stimuli.type(torch.float32), smap.type(torch.float32)\n",
    "    stimuli, smap = stimuli.to(device), smap.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs_1, outputs_2 = model(stimuli)\n",
    "        save_image(outputs_1[0], path_to_save_1+sample_batched['path'][0].split('/')[-1])\n",
    "        save_image(outputs_2[0], path_to_save_2+sample_batched['path'][0].split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_weight = 'your path to model here'\n",
    "path_to_save_1 = './datasets/val/maps_val_1/' # create folder maps_val_1 before\n",
    "path_to_save_2 = './datasets/val/maps_val_2/' # create folder maps_val_2 before\n",
    "model.load_state_dict(torch.load(path_to_weight))\n",
    "model.eval()\n",
    "\n",
    "for i_batch, sample_batched in tqdm(enumerate(dataloaders['val'])):\n",
    "    stimuli, smap = sample_batched['image'], sample_batched['saliency']\n",
    "    stimuli, smap = stimuli.type(torch.float32), smap.type(torch.float32)\n",
    "    stimuli, smap = stimuli.to(device), smap.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs_1, outputs_2 = model(stimuli)\n",
    "        save_image(outputs_1[0], path_to_save_1+sample_batched['path'][0].split('/')[-1])\n",
    "        save_image(outputs_2[0], path_to_save_2+sample_batched['path'][0].split('/')[-1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m104"
  },
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
