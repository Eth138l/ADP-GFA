{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Z3dyFZET9P90"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import bitsandbytes as bnb\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.loss_function import SaliencyLoss\n",
    "from utils.loss_function import AUC\n",
    "from utils.data_process import MyDataset, MyTransform\n",
    "from utils.data_process import MyDatasetCNNMerge, MyTransformCNNMerge\n",
    "from utils.data_process import preprocess_img, postprocess_img\n",
    "from utils.data_process import compute_metric, compute_metric_CNNMerge\n",
    "from utils.data_process import count_parameters\n",
    "\n",
    "from model.Merge_CNN_model import CNNMerge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OnIDauBqVlKY"
   },
   "outputs": [],
   "source": [
    "path_maps_1_train = './datasets/train/maps_train_1/'\n",
    "path_maps_2_train = './datasets/train/maps_train_2/'\n",
    "\n",
    "path_maps_1_val = './datasets/val/maps_val_1/'\n",
    "path_maps_2_val = './datasets/val/maps_val_2/'\n",
    "\n",
    "path_maps_train = './datasets/train/train_maps/'\n",
    "path_maps_val = './datasets/val/val_maps/'\n",
    "\n",
    "path_train_ids = './datasets/train_ids_SALICON_CAT.csv'\n",
    "path_val_ids = './datasets/val_ids_SALICON_CAT.csv'\n",
    "\n",
    "print(len(os.listdir(path_maps_1_train))), print(len(os.listdir(path_maps_2_train)))\n",
    "print(len(os.listdir(path_maps_1_val))), print(len(os.listdir(path_maps_2_val)))\n",
    "print(len(os.listdir(path_maps_train)))\n",
    "print(len(os.listdir(path_maps_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = pd.read_csv(path_train_ids)\n",
    "val_ids = pd.read_csv(path_val_ids)\n",
    "print(train_ids.iloc[1])\n",
    "print(val_ids.iloc[1])\n",
    "\n",
    "dataset_sizes = {'train': len(train_ids), 'val': len(val_ids)}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MNqBX79-VlKY"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "shape_r = 288\n",
    "shape_c = 384\n",
    "\n",
    "transform = MyTransformCNNMerge(shape_r=shape_r, shape_c=shape_c)\n",
    "\n",
    "train_set = MyDatasetCNNMerge(\n",
    "    ids=train_ids,\n",
    "    map1_dir=path_maps_1_train,\n",
    "    map2_dir=path_maps_2_train,\n",
    "    saliency_dir=path_maps_train,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_set = MyDatasetCNNMerge(\n",
    "    ids=val_ids,\n",
    "    map1_dir=path_maps_1_val,\n",
    "    map2_dir=path_maps_2_val,\n",
    "    saliency_dir=path_maps_val,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "dataloaders = {\n",
    "    'train':DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2), \n",
    "    'val':DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNMerge()\n",
    "model = model.to(device)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZR2aj0i9P96"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "H6dseWckHh_Y"
   },
   "outputs": [],
   "source": [
    "history_loss_train = []\n",
    "history_loss_val = []\n",
    "\n",
    "history_loss_train_cc = []\n",
    "history_loss_train_sim = []\n",
    "history_loss_train_kldiv = []\n",
    "history_loss_train_nss = []\n",
    "history_loss_train_auc = []\n",
    "\n",
    "history_loss_val_cc = []\n",
    "history_loss_val_sim = []\n",
    "history_loss_val_kldiv = []\n",
    "history_loss_val_nss = []\n",
    "history_loss_val_auc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "loss_fn = SaliencyLoss()\n",
    "\n",
    "'''Training'''\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "num_epochs = 30\n",
    "best_loss = 100\n",
    "path_to_save = 'path to save'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()  # Set model to training mode\n",
    "        else:\n",
    "            model.eval()   # Set model to evaluate mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for i_batch, sample_batched in tqdm(enumerate(dataloaders[phase])):\n",
    "            smap1, smap2, smap = sample_batched['map1'], sample_batched['map2'], sample_batched['saliency']\n",
    "            smap1, smap2, smap = smap1.type(torch.float32), smap2.type(torch.float32), smap.type(torch.float32)\n",
    "            smap1, smap2, smap = smap1.to(device), smap2.to(device), smap.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(smap1, smap2)\n",
    "\n",
    "                loss = -2*loss_fn(outputs, smap, loss_type='cc')\n",
    "                loss = loss - loss_fn(outputs, smap, loss_type='sim')\n",
    "                loss = loss + 10*loss_fn(outputs, smap, loss_type='kldiv')\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            if phase == 'train':\n",
    "                history_loss_train.append(loss.item())\n",
    "                history_loss_train_cc.append(loss_fn(outputs, smap, loss_type='cc').item())\n",
    "                history_loss_train_sim.append(loss_fn(outputs, smap, loss_type='sim').item())\n",
    "                history_loss_train_kldiv.append(loss_fn(outputs, smap, loss_type='kldiv').item())\n",
    "                history_loss_train_nss.append(loss_fn(outputs, smap, loss_type='nss').item())\n",
    "            else:\n",
    "                history_loss_val.append(loss.item())\n",
    "                history_loss_val_cc.append(loss_fn(outputs, smap, loss_type='cc').item())\n",
    "                history_loss_val_sim.append(loss_fn(outputs, smap, loss_type='sim').item())\n",
    "                history_loss_val_kldiv.append(loss_fn(outputs, smap, loss_type='kldiv').item())\n",
    "                history_loss_val_nss.append(loss_fn(outputs, smap, loss_type='nss').item())\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        if phase == 'train':\n",
    "            scheduler.step()\n",
    "\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "        print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "\n",
    "        if phase == 'val' and epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            counter = 0\n",
    "        elif phase == 'val' and epoch_loss >= best_loss:\n",
    "            counter += 1\n",
    "            if counter == 5:\n",
    "                savepath = path_to_save + '/CNNMerge_'+str(epoch)+'.pth'\n",
    "                torch.save(model.state_dict(), savepath)\n",
    "                print('EARLY STOP!')\n",
    "                break\n",
    "\n",
    "    # saving weights\n",
    "    if epoch%1 == 0:\n",
    "        savepath = path_to_save + '/CNNMerge_'+str(epoch)+'.pth'\n",
    "        torch.save(model.state_dict(), savepath)\n",
    "\n",
    "    print()\n",
    "\n",
    "print('Best val loss: {:4f}'.format(best_loss))\n",
    "savepath = path_to_save + '/CNNMerge_'+str(epoch)+'.pth'\n",
    "torch.save(model.state_dict(), savepath)\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geurEf_d_koh"
   },
   "source": [
    "# Show val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jBf7Ihie3Evk"
   },
   "outputs": [],
   "source": [
    "def make_sub(model, num_pic, shape_r=288, shape_c=384):\n",
    "    map1_path = './datasets/maps_val_1/COCO_val2014_'+num_pic+'.jpg'\n",
    "    map2_path = './datasets/maps_val_2/COCO_val2014_'+num_pic+'.jpg'\n",
    "    smap_path = './datasets/val_maps/COCO_val2014_'+num_pic+'.png'\n",
    "    \n",
    "    map1 = Image.open(map1_path).convert('L')\n",
    "    smap1 = np.expand_dims(np.array(map1) / 255., axis=0)\n",
    "    smap1 = torch.from_numpy(smap1)\n",
    "    \n",
    "    map2 = Image.open(map2_path).convert('L')\n",
    "    smap2 = np.expand_dims(np.array(map2) / 255., axis=0)\n",
    "    smap2 = torch.from_numpy(smap2)\n",
    "\n",
    "    saliency = Image.open(smap_path).convert('L')\n",
    "    smap = np.expand_dims(np.array(saliency) / 255., axis=0)\n",
    "    smap = torch.from_numpy(smap)\n",
    "\n",
    "    smap1, smap2, smap = transform(smap1, smap2, smap)\n",
    "    smap1 = smap1.type(torch.float32).to(device)\n",
    "    smap2 = smap2.type(torch.float32).to(device)\n",
    "    \n",
    "    toPIL = transforms.ToPILImage()\n",
    "    pred = model(smap1.unsqueeze(0), smap2.unsqueeze(0))\n",
    "    pic = toPIL(pred.squeeze())\n",
    "    pred_np = pred.squeeze().detach().cpu().numpy()\n",
    "    smp_np = smap.squeeze().numpy()\n",
    "    auc = AUC(pred_np, smp_np)\n",
    "    \n",
    "    return pic, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "path_sub_model = 'your path to model here'\n",
    "model.load_state_dict(torch.load(path_sub_model))\n",
    "model.eval()\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "num_pic = '000000102837'\n",
    "image = mpimg.imread('./datasets/val_images/COCO_val2014_'+num_pic+'.jpg')\n",
    "img_true = mpimg.imread('./datasets/val_maps/COCO_val2014_'+num_pic+'.png')\n",
    "pic, auc = make_sub(model, num_pic)\n",
    "\n",
    "ax[0].imshow(image)\n",
    "ax[0].set_title('Image')\n",
    "ax[1].imshow(img_true)\n",
    "ax[1].set_title('True')\n",
    "ax[2].imshow(pic)\n",
    "ax[2].set_title('Pred')\n",
    "\n",
    "plt.show()\n",
    "print('AUC = ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFDCtgvM3Evk"
   },
   "source": [
    "# Compute val metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sub_model = 'your path to model here'\n",
    "model.load_state_dict(torch.load(path_sub_model))\n",
    "model.eval()\n",
    "\n",
    "val_loss, val_loss_cc, val_loss_sim, val_loss_kldiv, val_loss_nss, val_auc = compute_metric_CNNMerge(\n",
    "    model, \n",
    "    dataloaders['val'], \n",
    "    device = device,\n",
    "    t=10\n",
    ")\n",
    "\n",
    "print('Loss = ', val_loss)\n",
    "print('CC = ', val_loss_cc)\n",
    "print('SIM = ', val_loss_sim)\n",
    "print('KL = ', val_loss_kldiv)\n",
    "print('NSS = ', val_loss_nss)\n",
    "print('AUC = ', val_auc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m104"
  },
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
